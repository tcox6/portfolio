<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Generalised Coordinate Learning | UTAS Research Project - Tim's Portfolio</title>
        <link rel="stylesheet" href="/defaultStyles/style.css">
        <link rel="stylesheet" href="/defaultStyles/projectDescriptionStyles.css">
        <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    </head>
    <body>
        <div class="mainContent" id="mainContent">
            <!--LOAD HEADER-->
            <div id="header"></div>
            <script src="/header/load-header.js"></script>

            <div class="background">
                <h1>Novel Reinforcement Learning Algorithm</h1>

                <p>This was a UTAS research project that I completed during the second year of my BICT, supervised by Dr. Robert Ollington. The project focussed on generalising an existing RL algorithm, within a navigational context (though it was theorised that, if successful, the new algorithm could be applied to any learning problem). The project had four distinct stages: a literature review; replicating the results of previous researchers (and developing a training environment to do so); showing that the existing algorithm of interest lacked efficacy when the training environment contained obstacles; and developing and testing the new algorithm. I also wrote an (unpublished) 20-page paper on the project.</p>

                <p>The training environment used for the project was inspired by experiments undertaken in the late 1990s. Specifically, the delayed matching-to-place watermaze (DMP environment) refers to an experiment where rats were placed into a circular tank of water with a hidden (submerged) escape platform. Rats were given several trials in the watermaze per day, and the location of the escape platform was changed each day. Researchers found that the rats exhibited 'one-trial' learning, where their escape latencies converged after a single trial in the altered watermaze.</p>

                <p>One-trial learning is difficult to achieve with traditional reinforcement learning algorithms since they are inhibited by learning from previous trials with inconsistent goal states; a goal-independent algorithm is needed. Goal-independent learning algorithms have been developed by various researchers, and my project focussed on generalising an existing algorithm where the agent learnt an encoding of its position within the watermaze, as opposed to learning a concrete mapping to the goal state. This existing algorithm was found to be effective in the standard DMP environment, but failed when the environment contained obstacles, hence the need for generalisation.</p>

                <p>Ultimately, our new algorithm was unsuccessful.</p>
            </div>

            <!--LOAD FOOTER-->
            <div id="footer"></div>
            <script src="/footer/load-footer.js"></script>
        </div>
        
        <!--LOAD GAME OF LIFE BACKGROUND-->
        <canvas id="gameOfLifeCanvas"></canvas>
        <script src="/scripts/game-of-life.js"></script>
    </body>
</html>