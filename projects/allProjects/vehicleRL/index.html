<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Vehicular RL - Tim's Portfolio</title>
        <link rel="stylesheet" href="/defaultStyles/style.css">
        <link rel="stylesheet" href="/defaultStyles/projectDescriptionStyles.css">
        <link rel="icon" href="/assets/favicon.ico" type="image/x-icon">
    </head>
    <body>
        <div class="mainContent" id="mainContent">
            <!--LOAD HEADER-->
            <div id="header"></div>
            <script src="/header/load-header.js"></script>

            <div class="background">
                <h1>Vehicular RL</h1>

                <p>This was a quick, yet involved project that I completed over the Easter mid-semester break in 2024. The goal was to build an environment editor and train agents to navigate through a customisable road network. I setup the project with the intention of making it a multi-agent learning environment (MARL), but haven't finished implementing this yet.</p>

                <p>I built the environment editor from the ground-up, which featured a drag-and-drop interface for placing road segments into a grid. I defined the road boundaries as parametric linear lines, and used these borders to facilitate collision-detection and sensor ray measurements for the agents. (The vehicle's sensor rays were also implemented as parametric linear lines, and thus the collision logic is relatively elementary.) The borders for road segments with curves were created as polygons, comprised of many linear segments. Road segments included straight sections, 90-degree curves, four-way intersections (cross intersections), and T-intersections.</p>

                <p>I used the DDPG algorithm to train the agents to navigate the road network, using sensor-ray inputs to measure the distance from the agent to the road borders. The DDPG algorithm is an actor-critic algorithm designed for continuous action spaces. It's an off-policy algorithm, meaning that it learns from a replay buffer, and uses target networks and soft updates to improve stability. Ultimately, I found that the DDPG algorithm with sensor-ray inputs was effective at solving the single-agent environment, though would sometimes fail when navigating cross-intersections.</p>

                <div style="width: 50%; margin-left: 25%;">
                    <div style="position: relative; padding-top: 100%;">
                        <iframe
                            src="https://customer-ain7slg8si2sx3xd.cloudflarestream.com/ff5426635fdd53c08386942b762463af/iframe?poster=https%3A%2F%2Fcustomer-ain7slg8si2sx3xd.cloudflarestream.com%2Fff5426635fdd53c08386942b762463af%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600"
                            loading="lazy"
                            style="border: none; position: absolute; top: 0; left: 0; height: 100%; width: 100%;"
                            allow="accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;"
                            allowfullscreen="true"
                        ></iframe>
                    </div>
                </div>
            </div>

            <!--LOAD FOOTER-->
            <div id="footer"></div>
            <script src="/footer/load-footer.js"></script>
        </div>
        
        <!--LOAD GAME OF LIFE BACKGROUND-->
        <canvas id="gameOfLifeCanvas"></canvas>
        <script src="/scripts/game-of-life.js"></script>
    </body>
</html>